<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>web scraping on Everyday Statistics</title>
    <link>https://SamLeeBYU.github.io/tags/web-scraping/</link>
    <description>Recent content in web scraping on Everyday Statistics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 29 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://SamLeeBYU.github.io/tags/web-scraping/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Web Scraping Using Selenium: Best Practices and Example</title>
      <link>https://SamLeeBYU.github.io/2023/09/29/selenium-best-practices/</link>
      <pubDate>Fri, 29 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://SamLeeBYU.github.io/2023/09/29/selenium-best-practices/</guid>
      <description>Introduction (For Those New to Selenium) Throughout my research experience, I have been often charged with scraping large data sets that only exist as embedded through a government portal or as partially accessible through passing through or entering form information. Through BeautifulSoup (which is a great library for sifting through the HTML text itself), and the requests library, static sites (such as Wikipedia) can be scraped quite easily, but the former examples present more of a challenge.</description>
    </item>
    
  </channel>
</rss>
